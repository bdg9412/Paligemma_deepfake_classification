# Paligemma_deepfake_classification
ë³¸ ë¦¬í¬ì§€í† ë¦¬ëŠ” [2024 Aifactory Geema hackathon](https://aifactory.space/task/2733/overview) ì•„ì´ë””ì–´ ê¸°íšì— ëŒ€í•œ êµ¬í˜„ ì†ŒìŠ¤ ì½”ë“œë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

## ê²½ì§„ëŒ€íšŒ ê²°ê³¼
- ëŒ€ìƒ ìˆ˜ìƒğŸ†  
![image](https://github.com/user-attachments/assets/91bbaeac-75c2-4fa5-bd94-031ed78e6a5a)

## Purpose  
- ìƒì„±í˜• AI ëª¨ë¸ì„ ë”¥í˜ì´í¬ ì´ë¯¸ì§€ ë¶„ë¥˜ì— íŠ¹í™”ë˜ë„ë¡ íŒŒì¸íŠœë‹ ìˆ˜í–‰  

## Data Format
```
{
    "image": "deepfake image or normal imgae",
    "label": "0(deepfake image) / 1(normal image)"
    "question": "Is this image made by AI?"
}
```
## Key Point
- MultiModal ëª¨ë¸ í™œìš©
- QLora ì ìš©

## Methods
- Model
    - google/paligemma-3b-pt-224 (ì½”ë© / L4ì—ì„œ í•™ìŠµ ì§„í–‰)   
        - Gemma ì¤‘ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” multimodal ëª¨ë¸ ì„ íƒ  
- Data Processing
    - question ì»¬ëŸ¼ì— ëŒ€í•˜ì—¬ ì§ì ‘ ìƒì„± ì§„í–‰
      - ë‹¨ìˆœ ì§ˆë¬¸ : step loss[0.135400] / ì •ë³´ ì¶”ê°€ ì§ˆë¬¸ : step loss[0.118300] 
    - question ì»¬ëŸ¼ì— ì‹¬í”Œí•œ ì •ë³´ ì¶”ê°€ì˜ ê²½ìš°, step lossëŠ” ì¤„ì–´ë“¤ì—ˆì§€ë§Œ ë¶„ë¥˜ ëŠ¥ë ¥ì€ í•˜ë½  
      - MM-CoT ê´€ë ¨ ë…¼ë¬¸ì—ì„œ LLMì— CoTë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ì‹ì˜ ì ‘ê·¼ ë²•ì€ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ì•ˆë˜ëŠ” ì •ë³´ í™•ì¸  
    - MM-CoT ë…¼ë¬¸ì˜ image feature ì •ë³´ë¥¼ ì£¼ëŠ” ê²ƒì— ì°©ì•ˆí•˜ì—¬ image captioning ì •ë³´ë¥¼ ëª¨ë¸ì˜ ì…ë ¥ì— ì¶”ê°€  
      - QLoraì˜ rê°’ì„ ì¦ëŒ€ì‹œì¼œ í‘œí˜„ë ¥ì„ ë†’ì´ëŠ” ì ‘ê·¼ë²•ê³¼ í”„ë¡¬í”„íŠ¸ êµ¬ì„± ë°©ì•ˆì˜ ë³€ê²½ì´ í•„ìš”í•œ ê²ƒì„ í™•ì¸  
      - í”„ë¡¬í”„íŠ¸ êµ¬ì„±ì— ë”°ë¼ step lossê°€ 0.186500ì—ì„œ 0.136900ê¹Œì§€ ë¹„êµì  í¬ê²Œ ë³€ê²½ë˜ëŠ” ê²ƒì„ í™•ì¸

## Results  
- Paligemmaë¥¼ í™œìš©í•˜ì—¬ Deepfake ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ ë° ì¶”ë¡  ì§„í–‰  
  - 0 (deepfake ì´ë¯¸ì§€) / 1 (normal ì´ë¯¸ì§€) ë¶„ë¥˜ í™•ì¸ ì™„ë£Œ (https://github.com/bdg9412/Paligemma_deepfake_classification/blob/main/Fine_tuned_Model_Inference_202409.ipynb)
  - 10 epoch í•™ìŠµ ì´í›„ 1000ê°œ ì´ë¯¸ì§€ ëŒ€ìƒ accuracy ì¸¡ì • ê²°ê³¼ : 94.80 (label ë¶„í¬ : {0: 541, 1: 459})  
   

     
## Reference
huggingface/transformers (https://github.com/huggingface/transformers)  
paligemma finetuning (https://github.com/merveenoyan/smol-vision)   
Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., & Smola, A. (n.d.). Multimodal Chain-of-Thought reasoning in language models. (https://arxiv.org/abs/2302.00923)  

## License  
Gemma 2 License: https://ai.google.dev/gemma/terms  
